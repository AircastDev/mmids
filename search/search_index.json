{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mmids Mmids (Multi-Media Ingestion and Distribution System) is a powerful, user friendly, open source live video workflow server. User Friendly Complex video workflows can be easily configured by non-developers Observable Logging with an emphasis on corelations. Easily pinpoint logs relevant to a single stream on a busy server Developer Friendly Trivially add new workflow logic, new network protocols, etc... all with your own open-source or proprietary components Fully Dynamic Push and pull mechanisms to start, stop, and update workflows on the fly without disruptions. What Can The Server Do? Today, mmids can: Receive audio/video via RTMP/s publishers Serve audio/video to RTMP/s based clients Ingest audio and video from external sources, such as a remote Perform on the fly transcoding of live video Generate HLS feeds for video Push live video out to external RTMP servers. The power of mmids comes from how all these type of systems can be composed together. Below is a fully working mmids configuration file: settings { ffmpeg_path c:\\ffmpeg\\bin\\ffmpeg.exe # location of ffmpeg executable tls_cert_path cert.pfx # The certificate to use for rtmps tls_cert_password abcd # password for the certificate http_api_port 9011 # port for the HTTP api } # This workflow allows RTMP publishers to send video to # `rtmp://server/basic_read/<stream_key>` for any stream key. These video streams # are then available to RTMP clients to watch on # `rtmp://server/basic_watch/<stream_key>, using the same stream key that the incoming # video came in on. workflow basic_read_watch { rtmp_receive rtmp_app=basic_read stream_key=* rtmp_watch rtmp_app=basic_watch stream_key=* } # This workflow demonstrates a more complex video workflow. # * We accept video via RTMP via `rtmp://server/receive/<stream_key>` # * An HLS feed is created for archival purposes to `c:\\temp\\hls\\archive` # * Another HLS feed is created for live preview purposes to `c:\\temp\\hls\\live` # * Surface the video to RTMP clients via `rtmp://server/preview/<stream_key>` # * Transcode the video to 640x480 at 1mbps # * Create an HLS feed of the transcoded video to `c:\\temp\\hls\\result` # * Allow RTMP clients to watch the transcoded feed on `rtmp://server/watch/<stream_key>` # * Finally, push the transcoded video to youtube's `abcdefg` stream key workflow transcode_test { rtmp_receive rtmp_app=receive stream_key=* ffmpeg_hls path=c:\\temp\\hls\\archive duration=2 count=0 ffmpeg_hls path=c:\\temp\\hls\\preview duration=2 count=5 rtmp_watch rtmp_app=preview stream_key=* ffmpeg_transcode vcodec=h264 acodec=aac h264_preset=ultrafast size=640x480 kbps=1000 ffmpeg_hls path=c:\\temp\\hls\\result duration=2 rtmp_watch rtmp_app=watch stream_key=* ffmpeg_push target=rtmp://a.rtmp.youtube.com/live2/abcdefg } # Simple workflow that shows multi-streaming capabilities. We take in a video stream, # expose it to RTMP clients for previewing, then push the video out to multiple other # external RTMP servers workflow multi_streaming { rtmp_receive rtmp_app=multistream stream_key=abc1234 rtmp_watch rtmp_app=multistream-watch stream_key=abc1234 ffmpeg_push target=rtmp://a.rtmp.youtube.com/live2/some-youtube-key ffmpeg_push target=rtmps://live-api-s.facebook.com/rtmp/some-facebook-stream-key } # Pull video from a live HLS feed and allow rtmp players to watch workflow pull_test { ffmpeg_pull location=https://ll-hls-test.apple.com/llhls1/multi.m3u8 stream_name=pull_test rtmp_watch rtmp_app=pull stream_key=pull }","title":"Home"},{"location":"#mmids","text":"Mmids (Multi-Media Ingestion and Distribution System) is a powerful, user friendly, open source live video workflow server. User Friendly Complex video workflows can be easily configured by non-developers Observable Logging with an emphasis on corelations. Easily pinpoint logs relevant to a single stream on a busy server Developer Friendly Trivially add new workflow logic, new network protocols, etc... all with your own open-source or proprietary components Fully Dynamic Push and pull mechanisms to start, stop, and update workflows on the fly without disruptions.","title":"Mmids"},{"location":"#what-can-the-server-do","text":"Today, mmids can: Receive audio/video via RTMP/s publishers Serve audio/video to RTMP/s based clients Ingest audio and video from external sources, such as a remote Perform on the fly transcoding of live video Generate HLS feeds for video Push live video out to external RTMP servers. The power of mmids comes from how all these type of systems can be composed together. Below is a fully working mmids configuration file: settings { ffmpeg_path c:\\ffmpeg\\bin\\ffmpeg.exe # location of ffmpeg executable tls_cert_path cert.pfx # The certificate to use for rtmps tls_cert_password abcd # password for the certificate http_api_port 9011 # port for the HTTP api } # This workflow allows RTMP publishers to send video to # `rtmp://server/basic_read/<stream_key>` for any stream key. These video streams # are then available to RTMP clients to watch on # `rtmp://server/basic_watch/<stream_key>, using the same stream key that the incoming # video came in on. workflow basic_read_watch { rtmp_receive rtmp_app=basic_read stream_key=* rtmp_watch rtmp_app=basic_watch stream_key=* } # This workflow demonstrates a more complex video workflow. # * We accept video via RTMP via `rtmp://server/receive/<stream_key>` # * An HLS feed is created for archival purposes to `c:\\temp\\hls\\archive` # * Another HLS feed is created for live preview purposes to `c:\\temp\\hls\\live` # * Surface the video to RTMP clients via `rtmp://server/preview/<stream_key>` # * Transcode the video to 640x480 at 1mbps # * Create an HLS feed of the transcoded video to `c:\\temp\\hls\\result` # * Allow RTMP clients to watch the transcoded feed on `rtmp://server/watch/<stream_key>` # * Finally, push the transcoded video to youtube's `abcdefg` stream key workflow transcode_test { rtmp_receive rtmp_app=receive stream_key=* ffmpeg_hls path=c:\\temp\\hls\\archive duration=2 count=0 ffmpeg_hls path=c:\\temp\\hls\\preview duration=2 count=5 rtmp_watch rtmp_app=preview stream_key=* ffmpeg_transcode vcodec=h264 acodec=aac h264_preset=ultrafast size=640x480 kbps=1000 ffmpeg_hls path=c:\\temp\\hls\\result duration=2 rtmp_watch rtmp_app=watch stream_key=* ffmpeg_push target=rtmp://a.rtmp.youtube.com/live2/abcdefg } # Simple workflow that shows multi-streaming capabilities. We take in a video stream, # expose it to RTMP clients for previewing, then push the video out to multiple other # external RTMP servers workflow multi_streaming { rtmp_receive rtmp_app=multistream stream_key=abc1234 rtmp_watch rtmp_app=multistream-watch stream_key=abc1234 ffmpeg_push target=rtmp://a.rtmp.youtube.com/live2/some-youtube-key ffmpeg_push target=rtmps://live-api-s.facebook.com/rtmp/some-facebook-stream-key } # Pull video from a live HLS feed and allow rtmp players to watch workflow pull_test { ffmpeg_pull location=https://ll-hls-test.apple.com/llhls1/multi.m3u8 stream_name=pull_test rtmp_watch rtmp_app=pull stream_key=pull }","title":"What Can The Server Do?"},{"location":"dev-guide/architecture/","text":"Architecture Mmids is built in the Rust programming language. There are two main projects that are relevant to developers: mmids-app contains the code that the official mmids distribution is built from. It is a good starting point for developers to see how all the components can be used together. mmids-core is the crate that contains the bulk of the logic and systems that mmids deploys. It is available on crates.io as a library TODO: add link. The code heavily relies on the tokio asynchronous runtime . Components Almost every component that makes up mmids s designed to be independent asynchronous actors. Each actor manages it's own state and different actors only talk to each other through message channels (usually UnboundedSender<T> channels). Replacing (or mocking) any component is usually a matter of creating custom code that can respond to incoming messages to the channel. graph TD eh[Event Hub] wm[Workflow Manager] rm[Reactor Manager] r[Reactors] w[Workflows] ws[Workflow Steps] sm[TCP Socket Manager] ep[Endpoints] http[HTTP Api] wm --> eh rm --> eh ws --> eh wm --> w w --> ws ws --> ep ws --> rm ep --> sm http --> wm rm --> r r --> wm r --> eh Endpoints Endpoints are actors which are abstract external communications for workflow steps. Most networking protocols and external system communication that would be shared between different workflow steps would be implemented as their own endpoints. This keeps the logic of workflow steps focused on the business logic for processing media, and the complexities of network protocols and process handling in a centralized location. For example, implementing an SRT server, creating HLS playlists, or managing outbound RTMP connections would all be implemented as endpoints. Mmids officially implements two endpoints: * Rtmp Server Endpoint - Allows opening ports and managing incoming RTMP client connection based on instructions by workflow steps. It handles managing the RTMP sessions, passing media it receives to the proper workflow steps, and receiving media from workflow steps and giving it to RTMP publish clients. * Ffmpeg endpoint - Allows creating ffmpeg processes with specific parameters, restarting the processes if they unexpectedly shut down, and shutting them down as requested. Workflow Manager The workflow manager is a central actor which holds a reference to all running workflows. Requests to start, stop, or update workflows usually goes to the workflow manager, as it's unlikely other components have a direct reference to the different workflow specific messaging channels. The workflow manager is also in charge of knowing when a workflow needs to be started and when an existing one needs to be updated instead. It is expected that any mmids system only has one workflow manager, as one workflow manager won't necessarily know about the workflows managed by the other. This can lead to some complicated scenarios, especially when workflow started events start being raised (e.g. other systems won't know which workflow manager to contact about a workflow). The workflow manager is started by calling the mmids_core::workflows::manager::start_workflow_manager() function. Workflows A workflow actor is started by the workflow manager by passing in a WorkflowDefinition value. This definition contains instructions for the workflow on what steps it should maintain. The workflow will create the workflow steps that are contained in the workflow definition and place them in pending status. Once all pending workflow steps change their state to active, all pending steps become active steps and the workflow will start flowing media from one step to the next. If a workflow step ever transitions to an error state, the whole workflow will transition to an error state and all workflow steps will be shut down. The workflow will be restarted if it receives a request to update with a new workflow definition. Workflow Steps Workflow steps are the only components that are not asynchronous . They are meant to be called synchronously by a workflow. If a workflow step requires an asynchronous action, it will create a boxed future with the asynchronous operation and return it as an output. The workflow that is in charge of hte step will track the future, and once the future has completed the result will be passed as an input to the workflow step. All workflow steps are expected to create an enum which represents the results of any future that the workflow step will need completed. This enum should implement the StepFutureResult trait, which allows the enum to be casted down from a StepFutureResult into the step specific enum. Reactor Manager The reactor manager is a central actor which keeps references and manages all known reactors. When a workflow step needs to make a request to a specific reactor, it reaches out to the reactor manager to send the reques to the correct reactor. It is expected that only one reactor manager is active. The reactor manager is started by calling the mmids_core::reactors::manager::start_reactor_manager() function. Reactors Each reactor is a separate actor which knows how to communicate with a single external system. When it executes a query for a stream name, and the external system responds with some workflows, the reactor will ensure that the workflows it created are shut down when the stream is over. If the reactor has been set with an update interval, it will continually re-execute queries against the external system for the stream name to ensure it's always managing the latest versions of the workflow that are expected for that stream. Each reactor contains a Reactor Executor, which is a struct that implements the mmids_core::reactors::executors::ReactorExecutor trait. The executor object is responsible for actually performing requests to the external systems on behalf of the reactor. Mmids only officially supports a simple_http executor, which is documented in the reactor section . When implementing a custom executor, the executor itself should handle retries as it sees fit, as the reactor has no retry logic in it. Either the executor returns a result that says it's valid, or a result that says it's invalid. Event Hub Event hub is a central actor that allows components to subscribe to events, and publish their own events. Currently this is mostly used for a workflow manager to raise a notification when it goes live (so the reactor manager knows how to contact it), and when workflows start and stop (so workflow forwarders know how to forward media to different workflows). It is expected that only a single event hub actor is running at any given time. HTTP API Mmids uses hyper to power its HTTP API. The core library does not do any routing, but instead it defines the mmids_core::http_api::routing::RouteHandler trait. Each implementation of a route handler represents a single set of logic that should be used to resolve a web request. Route handlers utilize the async_trait crate to provide an async return type. To utilize these routes, a mmids_core::http_api::routing::RoutingTable is created and route handler instances are registered with it for specific url paths. This allows for libraries to contain the route handlers logic in a central place, but allowing the final distribution to figure out what urls and HTTP methods should expose which route handlers (and which should not be enabled). This isn't a perfect abstraction though. Route URLs are defined as a Vec of PathPart values, where some path parts are representing a parameter instead of a strict value. So for example, /workflows/<name> woulc be built as vec![ PathPart::Exact { value: \"workflows\".to_string(), }, PathPart::Parameter { name: \"workflow\".to_string(), }, ] This specifies that the 2nd value is a parameter named workflow , and the route handler will be looking for a path parameter with that exact name. So any URL that this route handler is tied to must have at least one path part as a parameter named workflow . TCP Socket Manager The TCP socket manager is an actor to abstract the handling of TCP listeners. Systems can request that ports be opened for raw TCP connections or TLS connections, and it will handle letting the caller know when any connections connect.","title":"Architecture"},{"location":"dev-guide/architecture/#architecture","text":"Mmids is built in the Rust programming language. There are two main projects that are relevant to developers: mmids-app contains the code that the official mmids distribution is built from. It is a good starting point for developers to see how all the components can be used together. mmids-core is the crate that contains the bulk of the logic and systems that mmids deploys. It is available on crates.io as a library TODO: add link. The code heavily relies on the tokio asynchronous runtime .","title":"Architecture"},{"location":"dev-guide/architecture/#components","text":"Almost every component that makes up mmids s designed to be independent asynchronous actors. Each actor manages it's own state and different actors only talk to each other through message channels (usually UnboundedSender<T> channels). Replacing (or mocking) any component is usually a matter of creating custom code that can respond to incoming messages to the channel. graph TD eh[Event Hub] wm[Workflow Manager] rm[Reactor Manager] r[Reactors] w[Workflows] ws[Workflow Steps] sm[TCP Socket Manager] ep[Endpoints] http[HTTP Api] wm --> eh rm --> eh ws --> eh wm --> w w --> ws ws --> ep ws --> rm ep --> sm http --> wm rm --> r r --> wm r --> eh","title":"Components"},{"location":"dev-guide/architecture/#endpoints","text":"Endpoints are actors which are abstract external communications for workflow steps. Most networking protocols and external system communication that would be shared between different workflow steps would be implemented as their own endpoints. This keeps the logic of workflow steps focused on the business logic for processing media, and the complexities of network protocols and process handling in a centralized location. For example, implementing an SRT server, creating HLS playlists, or managing outbound RTMP connections would all be implemented as endpoints. Mmids officially implements two endpoints: * Rtmp Server Endpoint - Allows opening ports and managing incoming RTMP client connection based on instructions by workflow steps. It handles managing the RTMP sessions, passing media it receives to the proper workflow steps, and receiving media from workflow steps and giving it to RTMP publish clients. * Ffmpeg endpoint - Allows creating ffmpeg processes with specific parameters, restarting the processes if they unexpectedly shut down, and shutting them down as requested.","title":"Endpoints"},{"location":"dev-guide/architecture/#workflow-manager","text":"The workflow manager is a central actor which holds a reference to all running workflows. Requests to start, stop, or update workflows usually goes to the workflow manager, as it's unlikely other components have a direct reference to the different workflow specific messaging channels. The workflow manager is also in charge of knowing when a workflow needs to be started and when an existing one needs to be updated instead. It is expected that any mmids system only has one workflow manager, as one workflow manager won't necessarily know about the workflows managed by the other. This can lead to some complicated scenarios, especially when workflow started events start being raised (e.g. other systems won't know which workflow manager to contact about a workflow). The workflow manager is started by calling the mmids_core::workflows::manager::start_workflow_manager() function.","title":"Workflow Manager"},{"location":"dev-guide/architecture/#workflows","text":"A workflow actor is started by the workflow manager by passing in a WorkflowDefinition value. This definition contains instructions for the workflow on what steps it should maintain. The workflow will create the workflow steps that are contained in the workflow definition and place them in pending status. Once all pending workflow steps change their state to active, all pending steps become active steps and the workflow will start flowing media from one step to the next. If a workflow step ever transitions to an error state, the whole workflow will transition to an error state and all workflow steps will be shut down. The workflow will be restarted if it receives a request to update with a new workflow definition.","title":"Workflows"},{"location":"dev-guide/architecture/#workflow-steps","text":"Workflow steps are the only components that are not asynchronous . They are meant to be called synchronously by a workflow. If a workflow step requires an asynchronous action, it will create a boxed future with the asynchronous operation and return it as an output. The workflow that is in charge of hte step will track the future, and once the future has completed the result will be passed as an input to the workflow step. All workflow steps are expected to create an enum which represents the results of any future that the workflow step will need completed. This enum should implement the StepFutureResult trait, which allows the enum to be casted down from a StepFutureResult into the step specific enum.","title":"Workflow Steps"},{"location":"dev-guide/architecture/#reactor-manager","text":"The reactor manager is a central actor which keeps references and manages all known reactors. When a workflow step needs to make a request to a specific reactor, it reaches out to the reactor manager to send the reques to the correct reactor. It is expected that only one reactor manager is active. The reactor manager is started by calling the mmids_core::reactors::manager::start_reactor_manager() function.","title":"Reactor Manager"},{"location":"dev-guide/architecture/#reactors","text":"Each reactor is a separate actor which knows how to communicate with a single external system. When it executes a query for a stream name, and the external system responds with some workflows, the reactor will ensure that the workflows it created are shut down when the stream is over. If the reactor has been set with an update interval, it will continually re-execute queries against the external system for the stream name to ensure it's always managing the latest versions of the workflow that are expected for that stream. Each reactor contains a Reactor Executor, which is a struct that implements the mmids_core::reactors::executors::ReactorExecutor trait. The executor object is responsible for actually performing requests to the external systems on behalf of the reactor. Mmids only officially supports a simple_http executor, which is documented in the reactor section . When implementing a custom executor, the executor itself should handle retries as it sees fit, as the reactor has no retry logic in it. Either the executor returns a result that says it's valid, or a result that says it's invalid.","title":"Reactors"},{"location":"dev-guide/architecture/#event-hub","text":"Event hub is a central actor that allows components to subscribe to events, and publish their own events. Currently this is mostly used for a workflow manager to raise a notification when it goes live (so the reactor manager knows how to contact it), and when workflows start and stop (so workflow forwarders know how to forward media to different workflows). It is expected that only a single event hub actor is running at any given time.","title":"Event Hub"},{"location":"dev-guide/architecture/#http-api","text":"Mmids uses hyper to power its HTTP API. The core library does not do any routing, but instead it defines the mmids_core::http_api::routing::RouteHandler trait. Each implementation of a route handler represents a single set of logic that should be used to resolve a web request. Route handlers utilize the async_trait crate to provide an async return type. To utilize these routes, a mmids_core::http_api::routing::RoutingTable is created and route handler instances are registered with it for specific url paths. This allows for libraries to contain the route handlers logic in a central place, but allowing the final distribution to figure out what urls and HTTP methods should expose which route handlers (and which should not be enabled). This isn't a perfect abstraction though. Route URLs are defined as a Vec of PathPart values, where some path parts are representing a parameter instead of a strict value. So for example, /workflows/<name> woulc be built as vec![ PathPart::Exact { value: \"workflows\".to_string(), }, PathPart::Parameter { name: \"workflow\".to_string(), }, ] This specifies that the 2nd value is a parameter named workflow , and the route handler will be looking for a path parameter with that exact name. So any URL that this route handler is tied to must have at least one path part as a parameter named workflow .","title":"HTTP API"},{"location":"dev-guide/architecture/#tcp-socket-manager","text":"The TCP socket manager is an actor to abstract the handling of TCP listeners. Systems can request that ports be opened for raw TCP connections or TLS connections, and it will handle letting the caller know when any connections connect.","title":"TCP Socket Manager"},{"location":"dev-guide/custom-distribution/","text":"Custom Distribution When creating custom components, or wanting to cut down on available functionality in mmids for specific purposes, you will need to create your own mmids application. The mmids application is responsible for creating the tokio runtime, creating all actors that are desired to be run, defining any HTTP routes, then letting the application run until it is desired to exit (e.g. exit on ctrl+c ). You can use the official mmids application as a guide, but the general order of operations are: Start logging infrastructure Mmids uses the tracing crate for logging Start all required endpoints Since most workflow steps will need to interact with endpoints, they will need a reference to the already created channels at the time of their creation to function This will also include starting the TCP socket manager if an endpoint is created that needs it. Start the event hub A lot of different components will require the event hub, and thus it needs to be started early on Start reactor manager First a mmids_core::reactors::executors::ReactorExecutorFactory needs to be created, and any executors you wish to have available should be registered. Then the reactor manager can be started Finally, you can use the reactor manager's channel to create all the reactors that are desired to be created Register available steps Create a mmids_core::workflows::steps::factory::WorkflowStepFactory , and then register all workflow steps that should be included. Once all steps have been registered, wrap the factory in an Arc , to ensure it can be passed around as needed. Create workflow manager and initial workflows Now the workflow manager can be created, and the provided channel can be used to start any workflows that should be started immediately. Start the HTTP Api Now start the HTTP API. With those steps, everything should be ready to go.","title":"Building A Custom Distribution"},{"location":"dev-guide/custom-distribution/#custom-distribution","text":"When creating custom components, or wanting to cut down on available functionality in mmids for specific purposes, you will need to create your own mmids application. The mmids application is responsible for creating the tokio runtime, creating all actors that are desired to be run, defining any HTTP routes, then letting the application run until it is desired to exit (e.g. exit on ctrl+c ). You can use the official mmids application as a guide, but the general order of operations are: Start logging infrastructure Mmids uses the tracing crate for logging Start all required endpoints Since most workflow steps will need to interact with endpoints, they will need a reference to the already created channels at the time of their creation to function This will also include starting the TCP socket manager if an endpoint is created that needs it. Start the event hub A lot of different components will require the event hub, and thus it needs to be started early on Start reactor manager First a mmids_core::reactors::executors::ReactorExecutorFactory needs to be created, and any executors you wish to have available should be registered. Then the reactor manager can be started Finally, you can use the reactor manager's channel to create all the reactors that are desired to be created Register available steps Create a mmids_core::workflows::steps::factory::WorkflowStepFactory , and then register all workflow steps that should be included. Once all steps have been registered, wrap the factory in an Arc , to ensure it can be passed around as needed. Create workflow manager and initial workflows Now the workflow manager can be created, and the provided channel can be used to start any workflows that should be started immediately. Start the HTTP Api Now start the HTTP API. With those steps, everything should be ready to go.","title":"Custom Distribution"},{"location":"dev-guide/summary/","text":"Summary Mmids is open source, built in Rust, and made with extensibility and customization as a first class citizen. Useful Links Primary code repository mmids-core crate This crate is required for any extensibility or customization to mmids. It contains all the base library types, as well as all official workflow steps, endpoints, etc... API Reference Test Reactor Server A very simple server that can handle reactor requests with semi-hardcoded responses","title":"Useful Links"},{"location":"dev-guide/summary/#summary","text":"Mmids is open source, built in Rust, and made with extensibility and customization as a first class citizen.","title":"Summary"},{"location":"dev-guide/summary/#useful-links","text":"Primary code repository mmids-core crate This crate is required for any extensibility or customization to mmids. It contains all the base library types, as well as all official workflow steps, endpoints, etc... API Reference Test Reactor Server A very simple server that can handle reactor requests with semi-hardcoded responses","title":"Useful Links"},{"location":"user-guide/components/","text":"Components Mmids is based around several core components Workflow Steps A workflow step is a process that is instructed to ingest, distribute, or manipulate media. Some examples of functionality enabled by different workflow steps are: Receive RTMP video from publishers on specific stream keys Split media off to other workflows Instruct ffmpeg to transcode video to a new resolution Push media out to other servers via RTMP Workflows A workflow is a media pipeline defined by a linear sequence of workflow steps. When media is received by a workflow step, the media will be handed to the step defined afterwards for processing. There is no limit on the number of steps a single workflow can contain. Web Based API Mmids contains a web based API that can be used to query information about running workflows, as well as starting, stopping, and updating workflows on the fly. The web based API is the main way to make updates to a running mmids instance on a proactive (e.g. \"push\") basis. Warning Any change to workflows made via the web based API will not be saved. To persist workflow changes after a restart you will either need to update the mmids.config file, or re-send your workflow updates via the web API after restart. Reactors A reactor is a system which different workflow steps can ask about the validity of a stream, and request workflows to be created for that specific stream. When a reactor gets a query for a stream name, it will reach out to the configured external service to ask what workflows are valid for the specified stream name. If no workflows are returned then the reactor assumes the stream name is not valid and will send a response stating such. If at least one workflow is returned, then the reactor will create the specified workflows and tell the system that made the query that the stream name is valid, and which workflows it should route media on that stream to.","title":"Components"},{"location":"user-guide/components/#components","text":"Mmids is based around several core components","title":"Components"},{"location":"user-guide/components/#workflow-steps","text":"A workflow step is a process that is instructed to ingest, distribute, or manipulate media. Some examples of functionality enabled by different workflow steps are: Receive RTMP video from publishers on specific stream keys Split media off to other workflows Instruct ffmpeg to transcode video to a new resolution Push media out to other servers via RTMP","title":"Workflow Steps"},{"location":"user-guide/components/#workflows","text":"A workflow is a media pipeline defined by a linear sequence of workflow steps. When media is received by a workflow step, the media will be handed to the step defined afterwards for processing. There is no limit on the number of steps a single workflow can contain.","title":"Workflows"},{"location":"user-guide/components/#web-based-api","text":"Mmids contains a web based API that can be used to query information about running workflows, as well as starting, stopping, and updating workflows on the fly. The web based API is the main way to make updates to a running mmids instance on a proactive (e.g. \"push\") basis. Warning Any change to workflows made via the web based API will not be saved. To persist workflow changes after a restart you will either need to update the mmids.config file, or re-send your workflow updates via the web API after restart.","title":"Web Based API"},{"location":"user-guide/components/#reactors","text":"A reactor is a system which different workflow steps can ask about the validity of a stream, and request workflows to be created for that specific stream. When a reactor gets a query for a stream name, it will reach out to the configured external service to ask what workflows are valid for the specified stream name. If no workflows are returned then the reactor assumes the stream name is not valid and will send a response stating such. If at least one workflow is returned, then the reactor will create the specified workflows and tell the system that made the query that the stream name is valid, and which workflows it should route media on that stream to.","title":"Reactors"},{"location":"user-guide/configuration/","text":"Configuration Mmids is configured by writing into a file named mmids.config . The configuration syntax is a custom node based syntax, inspired by the NGINX configuration format. In general, you write configurations by defining nodes, giving those nodes arguments, and then define child nodes within the root nodes. The basic concept is: <node type> <arguments> { <child-node> <arguments> <child-node> <arguments> ... } Node types are predefined, and the type of arguments supported by each depends on the node itself. Any line can contain comments by placing a number sign (#) and the comments after it. All characters after the number sign are treated as comments and ignored until it reaches the end of the line. Settings Node Only one setting node is allowed, and the node itself has no arguments. Inside the setting node, each setting should be specified followed by a single optional (depending on the setting being specified) argument. Valid settings are: ffmpeg_path - This is the relative or absolute path to the ffmpeg executable. This setting is required for mmids to run. http_api_port - This is the port that the HTTP API will run on. If not specified than the HTTP API will be disabled tls_cert_path - This is the relative or absolute path to where a pfx certificate can be found. This certificate will be used for RTMPS connections. If not specified than RTMPS support will be disabled. tls_cert_password - This is the password that can be used to open the pfx certificate. If not specified than RTMPS support will be disabled An example settings configuration would be settings { ffmpeg_path c:\\tools\\ffmpeg\\bin\\ffmpeg.exe tls_cert_path cert.pfx tls_cert_password abcd http_api_port 9011 } Reactor Node Multiple reactor nodes can be specified. This is mostly meant to allow for different URLs to be accessed in different circumstances. All reactor configurations in the official mmids application will have the following look reactor <name> executor=simple_http update_interval=<interval> { url <url> } <name> - The name for this reactor. The name is used so workflow steps know which reactor to send queries for. Every reactor must have a unique name. Names can-not have spaces in them. <interval> - How many seconds until the reactor should execute another query. This is used for a reactor to auto-update workflows after it has started managing them. An update interval of 0 disables auto-updating. <url> - This is the full URL the reactor should use for queries. Workflow Node Multiple workflow nodes can be specified, with workflow steps defined as their child nodes. Workflow nodes are configured as: workflow <name> { <steps> } <name> - the name to give to the workflow. Every defined workflow must have a unique name. This name will be the same used when querying or modifying the workflow via the HTTP API. <steps> - One or more workflow steps that this workflow should contain. The order in which steps are defined dictate the order in which media will be processed. For example, placing a step to allow video playback before a transcode step will cause the pre-transcoded video to be played back, while placing the playback step after the transcode step will cause the transcoded video to be played back. Workflow Steps Each workflow step is configured in the following format: <step type> <arguments> <step_type> - This is the name of the step to be used. The names of each step are predetermined based on the workflow step. <arguments> - One or more arguments that are specific to the step being requested. For details on how to configure any specific step, see the workflow steps documentation .","title":"Configuration"},{"location":"user-guide/configuration/#configuration","text":"Mmids is configured by writing into a file named mmids.config . The configuration syntax is a custom node based syntax, inspired by the NGINX configuration format. In general, you write configurations by defining nodes, giving those nodes arguments, and then define child nodes within the root nodes. The basic concept is: <node type> <arguments> { <child-node> <arguments> <child-node> <arguments> ... } Node types are predefined, and the type of arguments supported by each depends on the node itself. Any line can contain comments by placing a number sign (#) and the comments after it. All characters after the number sign are treated as comments and ignored until it reaches the end of the line.","title":"Configuration"},{"location":"user-guide/configuration/#settings-node","text":"Only one setting node is allowed, and the node itself has no arguments. Inside the setting node, each setting should be specified followed by a single optional (depending on the setting being specified) argument. Valid settings are: ffmpeg_path - This is the relative or absolute path to the ffmpeg executable. This setting is required for mmids to run. http_api_port - This is the port that the HTTP API will run on. If not specified than the HTTP API will be disabled tls_cert_path - This is the relative or absolute path to where a pfx certificate can be found. This certificate will be used for RTMPS connections. If not specified than RTMPS support will be disabled. tls_cert_password - This is the password that can be used to open the pfx certificate. If not specified than RTMPS support will be disabled An example settings configuration would be settings { ffmpeg_path c:\\tools\\ffmpeg\\bin\\ffmpeg.exe tls_cert_path cert.pfx tls_cert_password abcd http_api_port 9011 }","title":"Settings Node"},{"location":"user-guide/configuration/#reactor-node","text":"Multiple reactor nodes can be specified. This is mostly meant to allow for different URLs to be accessed in different circumstances. All reactor configurations in the official mmids application will have the following look reactor <name> executor=simple_http update_interval=<interval> { url <url> } <name> - The name for this reactor. The name is used so workflow steps know which reactor to send queries for. Every reactor must have a unique name. Names can-not have spaces in them. <interval> - How many seconds until the reactor should execute another query. This is used for a reactor to auto-update workflows after it has started managing them. An update interval of 0 disables auto-updating. <url> - This is the full URL the reactor should use for queries.","title":"Reactor Node"},{"location":"user-guide/configuration/#workflow-node","text":"Multiple workflow nodes can be specified, with workflow steps defined as their child nodes. Workflow nodes are configured as: workflow <name> { <steps> } <name> - the name to give to the workflow. Every defined workflow must have a unique name. This name will be the same used when querying or modifying the workflow via the HTTP API. <steps> - One or more workflow steps that this workflow should contain. The order in which steps are defined dictate the order in which media will be processed. For example, placing a step to allow video playback before a transcode step will cause the pre-transcoded video to be played back, while placing the playback step after the transcode step will cause the transcoded video to be played back.","title":"Workflow Node"},{"location":"user-guide/configuration/#workflow-steps","text":"Each workflow step is configured in the following format: <step type> <arguments> <step_type> - This is the name of the step to be used. The names of each step are predetermined based on the workflow step. <arguments> - One or more arguments that are specific to the step being requested. For details on how to configure any specific step, see the workflow steps documentation .","title":"Workflow Steps"},{"location":"user-guide/http-api/","text":"HTTP API Mmids provides an HTTP based API for health checking and modification purposes. It is the \"push\" method of changing workflows, allowing you to start, stop, and update workflows as needed. The API runs on the port specified by the http_port setting value. If no port is specified, or the setting is not provided, then the HTTP API will be disabled. The API is bound to 127.0.0.1 , and thus is not accessible from external machines. GET / GET requests to the root ( / ) return information about the version of mmids that's currently running. It also works to act as a health check to know if mmids is currently running or not. GET /workflows GET requests to /workflows will return a JSON array of workflows that are currently running within mmids. GET /workflows/<name> GET requests to /workflows/<name> , where <name> is the name of a workflow, will return details about that workflow in JSON format. It will provide the current status of the workflow (e.g. Running or error details), which steps are active, and which steps are pending. Steps pending mean they are waiting for some action to be completed, such as registration with another system (e.g. the RTMP subsystem). It's possible that a pending task can cause a workflow to enter an error'd state, and in this case this API call will make that clear. If the workflow does not exist, than a 400 Not Found will be returned. PUT /workflows PUT requests to /workflows allows starting or updating a single workflow. The definition of a workflow is specified in the HTTP request body in the same configuration format as specified in the mmids.config file see the workflow node section for more info . If the workflow specified in the HTTP request body already exists, then the workflow will be updated to match what was requested. Any workflow steps that currently exist but were not in the passed in workflow definition will be removed, and any workflow steps that are new will be created. Note If a workflow step is currently active, and the new workflow definition passed in the HTTP request has the step with the same exact parameters , then the step will be kept and not be recreated. This means that an rtmp_receive and rtmp_watch step with the same exact parameters will not disconnect any clients that are active publishing or watching streams. However, if any parameters are added to this step, the step will be recreated and thus all current connections to those steps will be disconnected. Warning When inserting or removing workflow steps that come before other steps, you should be careful to understand when encoding parameters may be changed. If you are transcoding a media stream before sending them to playback clients and you remove the transcode step, the playback clients will receive new video and audio h264 headers for the pre-transcoded media feed. Many systems do not handle a change of encoding parameters mid stream and may not properly decode subsequent video calls. Thus if adding or removing transcoding steps is desired, then downstream steps should be modified as well to ensure correct operations. Note It's important to track if a workflow was created by a reactor before updating it. If a reactor is managing the specific workflow and you change it, the reactor may update it again to put it back in it's previous state. DELETE /workflows/<name> DELETE requests to /workflows/<name> , where <name> is the name of a workflow, will cause the workflow with the specified name to be stopped and all clients utilizing steps within that workflow will be removed. Note Deleting a workflow managed by a reactor may only be temprorary, as the reactor may end up re-creating the workflow again.","title":"HTTP API"},{"location":"user-guide/http-api/#http-api","text":"Mmids provides an HTTP based API for health checking and modification purposes. It is the \"push\" method of changing workflows, allowing you to start, stop, and update workflows as needed. The API runs on the port specified by the http_port setting value. If no port is specified, or the setting is not provided, then the HTTP API will be disabled. The API is bound to 127.0.0.1 , and thus is not accessible from external machines.","title":"HTTP API"},{"location":"user-guide/http-api/#get","text":"GET requests to the root ( / ) return information about the version of mmids that's currently running. It also works to act as a health check to know if mmids is currently running or not.","title":"GET /"},{"location":"user-guide/http-api/#get-workflows","text":"GET requests to /workflows will return a JSON array of workflows that are currently running within mmids.","title":"GET /workflows"},{"location":"user-guide/http-api/#get-workflowsname","text":"GET requests to /workflows/<name> , where <name> is the name of a workflow, will return details about that workflow in JSON format. It will provide the current status of the workflow (e.g. Running or error details), which steps are active, and which steps are pending. Steps pending mean they are waiting for some action to be completed, such as registration with another system (e.g. the RTMP subsystem). It's possible that a pending task can cause a workflow to enter an error'd state, and in this case this API call will make that clear. If the workflow does not exist, than a 400 Not Found will be returned.","title":"GET /workflows/&lt;name&gt;"},{"location":"user-guide/http-api/#put-workflows","text":"PUT requests to /workflows allows starting or updating a single workflow. The definition of a workflow is specified in the HTTP request body in the same configuration format as specified in the mmids.config file see the workflow node section for more info . If the workflow specified in the HTTP request body already exists, then the workflow will be updated to match what was requested. Any workflow steps that currently exist but were not in the passed in workflow definition will be removed, and any workflow steps that are new will be created. Note If a workflow step is currently active, and the new workflow definition passed in the HTTP request has the step with the same exact parameters , then the step will be kept and not be recreated. This means that an rtmp_receive and rtmp_watch step with the same exact parameters will not disconnect any clients that are active publishing or watching streams. However, if any parameters are added to this step, the step will be recreated and thus all current connections to those steps will be disconnected. Warning When inserting or removing workflow steps that come before other steps, you should be careful to understand when encoding parameters may be changed. If you are transcoding a media stream before sending them to playback clients and you remove the transcode step, the playback clients will receive new video and audio h264 headers for the pre-transcoded media feed. Many systems do not handle a change of encoding parameters mid stream and may not properly decode subsequent video calls. Thus if adding or removing transcoding steps is desired, then downstream steps should be modified as well to ensure correct operations. Note It's important to track if a workflow was created by a reactor before updating it. If a reactor is managing the specific workflow and you change it, the reactor may update it again to put it back in it's previous state.","title":"PUT /workflows"},{"location":"user-guide/http-api/#delete-workflowsname","text":"DELETE requests to /workflows/<name> , where <name> is the name of a workflow, will cause the workflow with the specified name to be stopped and all clients utilizing steps within that workflow will be removed. Note Deleting a workflow managed by a reactor may only be temprorary, as the reactor may end up re-creating the workflow again.","title":"DELETE /workflows/&lt;name&gt;"},{"location":"user-guide/quick-start/","text":"Quick Start Download mmids application Official releases of the mmids server can be downloaded from the official releases page . Extract the files in the whatever directory you wish to run it from, e.g. c:\\mmids. Download an FFMPEG executable Download a release of ffmpeg from their official site and place it in any location. Ffmpeg is currently required to pull, transcode, push, and create HLS feeds. Configuration Navigate to the directory the mmids release was extracted to and create a new file called mmids.config . Open the file in any text editor and set the context to the following: settings { http_api_port 9011 # Replace <path-to-ffmpeg.exe> with the path to the ffmpeg downloaded ffmpeg_path <path-to-ffmpeg.exe> } workflow simple { rtmp_receive rtmp_app=publish stream_key=* rtmp_watch rtmp_app=watch stream_key=* } Run mmids Navigate to the directory you created the mmids.config file into and run mmids-app.exe . Note Running the executable from within a command prompt is preferred when making configuration changes to ensure visiblity of configuration errors. You should now see the console filling with messages and should see a message such as: 2021-12-31T17:57:26.706818Z INFO mmids_core::workflows::runner: All pending steps moved to active at C:\\Users\\me\\code\\mmids\\mmids-core\\mmids-core\\src\\workflows\\runner\\mod.rs:594 in mmids_core::workflows::runner::Workflow Execution with , workflow_name: simple Verification via HTTP API Open a browser to http://localhost:9011 . You should see: Mmids version x.x.x Next you can navigate to http://localhost:9011/workflows , and you should see your single defined \"simple\" workflow running. [ { \"name\": \"simple\" } ] Finally, you can get details about the running workflow by navigating to http://localhost:9011/workflows/simple , which should show { \"status\": \"Running\", \"active_steps\": [ { \"step_id\": \"17261577973137769032\", \"step_type\": \"rtmp_receive\", \"parameters\": { \"rtmp_app\": \"publish\", \"stream_key\": \"*\" }, \"status\": \"Active\" }, { \"step_id\": \"8917233449957578608\", \"step_type\": \"rtmp_watch\", \"parameters\": { \"rtmp_app\": \"watch\", \"stream_key\": \"*\" }, \"status\": \"Active\" } ], \"pending_steps\": [] } Publishing and Watching Video Open up your favorite encoder and send video to rtmp://localhost/publish/test . You can then load any video player that supports RTMP (such as VLC) and watch the published stream by connecting to rtmp://localhost/watch/test .","title":"Quick Start"},{"location":"user-guide/quick-start/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/quick-start/#download-mmids-application","text":"Official releases of the mmids server can be downloaded from the official releases page . Extract the files in the whatever directory you wish to run it from, e.g. c:\\mmids.","title":"Download mmids application"},{"location":"user-guide/quick-start/#download-an-ffmpeg-executable","text":"Download a release of ffmpeg from their official site and place it in any location. Ffmpeg is currently required to pull, transcode, push, and create HLS feeds.","title":"Download an FFMPEG executable"},{"location":"user-guide/quick-start/#configuration","text":"Navigate to the directory the mmids release was extracted to and create a new file called mmids.config . Open the file in any text editor and set the context to the following: settings { http_api_port 9011 # Replace <path-to-ffmpeg.exe> with the path to the ffmpeg downloaded ffmpeg_path <path-to-ffmpeg.exe> } workflow simple { rtmp_receive rtmp_app=publish stream_key=* rtmp_watch rtmp_app=watch stream_key=* }","title":"Configuration"},{"location":"user-guide/quick-start/#run-mmids","text":"Navigate to the directory you created the mmids.config file into and run mmids-app.exe . Note Running the executable from within a command prompt is preferred when making configuration changes to ensure visiblity of configuration errors. You should now see the console filling with messages and should see a message such as: 2021-12-31T17:57:26.706818Z INFO mmids_core::workflows::runner: All pending steps moved to active at C:\\Users\\me\\code\\mmids\\mmids-core\\mmids-core\\src\\workflows\\runner\\mod.rs:594 in mmids_core::workflows::runner::Workflow Execution with , workflow_name: simple","title":"Run mmids"},{"location":"user-guide/quick-start/#verification-via-http-api","text":"Open a browser to http://localhost:9011 . You should see: Mmids version x.x.x Next you can navigate to http://localhost:9011/workflows , and you should see your single defined \"simple\" workflow running. [ { \"name\": \"simple\" } ] Finally, you can get details about the running workflow by navigating to http://localhost:9011/workflows/simple , which should show { \"status\": \"Running\", \"active_steps\": [ { \"step_id\": \"17261577973137769032\", \"step_type\": \"rtmp_receive\", \"parameters\": { \"rtmp_app\": \"publish\", \"stream_key\": \"*\" }, \"status\": \"Active\" }, { \"step_id\": \"8917233449957578608\", \"step_type\": \"rtmp_watch\", \"parameters\": { \"rtmp_app\": \"watch\", \"stream_key\": \"*\" }, \"status\": \"Active\" } ], \"pending_steps\": [] }","title":"Verification via HTTP API"},{"location":"user-guide/quick-start/#publishing-and-watching-video","text":"Open up your favorite encoder and send video to rtmp://localhost/publish/test . You can then load any video player that supports RTMP (such as VLC) and watch the published stream by connecting to rtmp://localhost/watch/test .","title":"Publishing and Watching Video"},{"location":"user-guide/reactors/","text":"Reactors Reactors create and manage workflows in reactions to new streams, and is the \"pull\" mechanism for dynamic workflows in mmids. Certain workflow steps have the (optional) ability to request that a reactor create workflows for a given stream name. The current workflow steps that support this so far are the RTMP receive , RTMP watch , and the Workflow forwarder steps. When a reactor receives a request to create workflows for a specific stream name, the reactor will make a call to an external system based on how that reactor was configured. Request Execution The method that reactors call external systems are called Reactor Executors . The official mmids distribution only contains a single executor, called simple_http . This executor will make an HTTP POST call to the url set in the reactor's configuration. The HTTP request will have a content type of text/plain and the body will only contain the name of the stream being queried. The simple_http executor expects the server to respond with: 404 - The stream name is not valid or allowed 200 - The stream name is valid and allowed (even if no workflows are returned) Note The simple_http executor has simple retry logic, where if it receives a status code that's not 400 or 200 it will try again 2 more times, once after 5 seconds and again after another 15 seconds. It will consider the stream as not valid if the 3rd retry failse. In order to respond to the executor with workflows, the target server must respond with one or more workflows [defined the same way you would in the configuration(configuration.md#Workflow%20Node)], with one small addition. Workflows defined as responses to reactor queries come in two types, routable and non-routable. Routable workflows have an argument routed_by_reactor added to the workflow node. When the reactor sees a workflow as routable, it will return the name of that workflow in it's response to the workflow step that sent the request to the reactor. This instructs some workflow steps (such as the workflow forwarder) on where to redirect media streams with this stream name. An example of a reactor response would be: workflow abc_watch routed_by_reactor { rtmp_watch rtmp_app=watch stream_key=abc } workflow abc_ingest { ffmpeg_pull location=https://ll-hls-test.apple.com/llhls1/multi.m3u8 stream_name=abc workflow_forwarder target_workflow=original_workflow } This will have the reactor create two workflows, one named abc_ingest and another abc_watch . The original workflow that called the reactor will only forward its media streams to abc_watch since abc_ingest is not marked as routed_by_reactor . In most cases reactors will respond with worklows with routed_by_reactor enabled, but some advanced configurations such as the above can be used for viewer load balancing, where you only ingest media from the source when there is an active watcher. Warning It is important to make sure that reactors return workflows with unique names for different stream names. If two stream names cause reactors to manage the same workflow name, then it's possible that the workflow can change or be stopped unexpectedly. Auto Updating When a reactor is configured with a update_interval argument that's greater than zero, the reactor will re-run execution based on the interval's value (in seconds) until the stream that requested it is gone. This allows the workflow to dynamically change while the stream is active, including stopping any workflows that the external system decides is no longer valid after it has begun.","title":"Reactors"},{"location":"user-guide/reactors/#reactors","text":"Reactors create and manage workflows in reactions to new streams, and is the \"pull\" mechanism for dynamic workflows in mmids. Certain workflow steps have the (optional) ability to request that a reactor create workflows for a given stream name. The current workflow steps that support this so far are the RTMP receive , RTMP watch , and the Workflow forwarder steps. When a reactor receives a request to create workflows for a specific stream name, the reactor will make a call to an external system based on how that reactor was configured.","title":"Reactors"},{"location":"user-guide/reactors/#request-execution","text":"The method that reactors call external systems are called Reactor Executors . The official mmids distribution only contains a single executor, called simple_http . This executor will make an HTTP POST call to the url set in the reactor's configuration. The HTTP request will have a content type of text/plain and the body will only contain the name of the stream being queried. The simple_http executor expects the server to respond with: 404 - The stream name is not valid or allowed 200 - The stream name is valid and allowed (even if no workflows are returned) Note The simple_http executor has simple retry logic, where if it receives a status code that's not 400 or 200 it will try again 2 more times, once after 5 seconds and again after another 15 seconds. It will consider the stream as not valid if the 3rd retry failse. In order to respond to the executor with workflows, the target server must respond with one or more workflows [defined the same way you would in the configuration(configuration.md#Workflow%20Node)], with one small addition. Workflows defined as responses to reactor queries come in two types, routable and non-routable. Routable workflows have an argument routed_by_reactor added to the workflow node. When the reactor sees a workflow as routable, it will return the name of that workflow in it's response to the workflow step that sent the request to the reactor. This instructs some workflow steps (such as the workflow forwarder) on where to redirect media streams with this stream name. An example of a reactor response would be: workflow abc_watch routed_by_reactor { rtmp_watch rtmp_app=watch stream_key=abc } workflow abc_ingest { ffmpeg_pull location=https://ll-hls-test.apple.com/llhls1/multi.m3u8 stream_name=abc workflow_forwarder target_workflow=original_workflow } This will have the reactor create two workflows, one named abc_ingest and another abc_watch . The original workflow that called the reactor will only forward its media streams to abc_watch since abc_ingest is not marked as routed_by_reactor . In most cases reactors will respond with worklows with routed_by_reactor enabled, but some advanced configurations such as the above can be used for viewer load balancing, where you only ingest media from the source when there is an active watcher. Warning It is important to make sure that reactors return workflows with unique names for different stream names. If two stream names cause reactors to manage the same workflow name, then it's possible that the workflow can change or be stopped unexpectedly.","title":"Request Execution"},{"location":"user-guide/reactors/#auto-updating","text":"When a reactor is configured with a update_interval argument that's greater than zero, the reactor will re-run execution based on the interval's value (in seconds) until the stream that requested it is gone. This allows the workflow to dynamically change while the stream is active, including stopping any workflows that the external system decides is no longer valid after it has begun.","title":"Auto Updating"},{"location":"user-guide/scenarios/complex_multistream/","text":"Complex Multi-Streaming In this scenario, a publisher wishes to send in video. The raw video should be sent to Youtube and Twitch as-is (to let them do their own transcoding), but we want to send a 720p feed to service A and 1080p feed to service B. graph TD A[Video Publisher] B[mmids] C[Youtube] D[Twitch] E[Service A] F[Service B] G[720p Transcode] H[1080p Transcode] A -->|rtmp://server/publish/stream| B B --> C B --> D B --> G G --> E B --> H H --> F This can be accomplished with the following configuration: workflow ingest { rtmp_receive rtmp_app=publish stream_key=stream workflow_forwarder target_workflow=transcode_720 workflow_forwarder target_workflow=transcode_1080 ffmpeg_push target=rtmp://a.rtmp.youtube.com/live2/some_yt_key ffmpeg_push target=rtmp://live.twitch.tv/app/some_twitch_key } workflow transcode_720 { ffmpeg_transcode vcodec=h264 acodec=copy size=1280x720 bitrate=2500 ffmpeg_push target=rtmp://service_a/app/some_key } workflow transcode_1080 { ffmpeg_transcode vcodec=h264 acodec=copy size=1920x1080 bitrate=4000 ffmpeg_push target=rtmp//service_b/app/some_key }","title":"Complex Multi-Streaming"},{"location":"user-guide/scenarios/complex_multistream/#complex-multi-streaming","text":"In this scenario, a publisher wishes to send in video. The raw video should be sent to Youtube and Twitch as-is (to let them do their own transcoding), but we want to send a 720p feed to service A and 1080p feed to service B. graph TD A[Video Publisher] B[mmids] C[Youtube] D[Twitch] E[Service A] F[Service B] G[720p Transcode] H[1080p Transcode] A -->|rtmp://server/publish/stream| B B --> C B --> D B --> G G --> E B --> H H --> F This can be accomplished with the following configuration: workflow ingest { rtmp_receive rtmp_app=publish stream_key=stream workflow_forwarder target_workflow=transcode_720 workflow_forwarder target_workflow=transcode_1080 ffmpeg_push target=rtmp://a.rtmp.youtube.com/live2/some_yt_key ffmpeg_push target=rtmp://live.twitch.tv/app/some_twitch_key } workflow transcode_720 { ffmpeg_transcode vcodec=h264 acodec=copy size=1280x720 bitrate=2500 ffmpeg_push target=rtmp://service_a/app/some_key } workflow transcode_1080 { ffmpeg_transcode vcodec=h264 acodec=copy size=1920x1080 bitrate=4000 ffmpeg_push target=rtmp//service_b/app/some_key }","title":"Complex Multi-Streaming"},{"location":"user-guide/scenarios/hls_publishing/","text":"HLS Publishing In this scenario we want publishers to be able to send video via RTMP into a mmids instance, and a CDN serve the created HLS feeds to viewers. graph TD Pub[Video Publisher] mmids Cdn folder[Filesystem] v1[Viewer 1] v2[Viewer 2] v3[Viewer 3] Pub -->|rtmp://server/publish/stream1| mmids mmids -->|/var/www/hls/stream1.m3u8| folder folder --> Cdn Cdn --> v1 Cdn -->|https://some-url/path/stream1.m3u8| v2 Cdn --> v3 This can be accomplished with the following configuration: workflow hls { rtmp_receive rtmp_app=publis stream_key=* ffmpeg_hls path=/var/www/hls duration=2 count=5 }","title":"HLS Publishing"},{"location":"user-guide/scenarios/hls_publishing/#hls-publishing","text":"In this scenario we want publishers to be able to send video via RTMP into a mmids instance, and a CDN serve the created HLS feeds to viewers. graph TD Pub[Video Publisher] mmids Cdn folder[Filesystem] v1[Viewer 1] v2[Viewer 2] v3[Viewer 3] Pub -->|rtmp://server/publish/stream1| mmids mmids -->|/var/www/hls/stream1.m3u8| folder folder --> Cdn Cdn --> v1 Cdn -->|https://some-url/path/stream1.m3u8| v2 Cdn --> v3 This can be accomplished with the following configuration: workflow hls { rtmp_receive rtmp_app=publis stream_key=* ffmpeg_hls path=/var/www/hls duration=2 count=5 }","title":"HLS Publishing"},{"location":"user-guide/scenarios/key_restricted/","text":"Restricted Publish / Playback By Stream Key In this scenario, we want to only allow one specific stream key to receive video. We want viewers to be able to connect and watch the stream, however we do not want viewers to know the stream key used to publish video (so others can't publish video as the publisher). Thus the publishing stream key is different than the watching stream key. graph TD A[Video Publisher] -->|rtmp://server/publish/abc| B[mmids] B -->|rtmp://server/watch/def| C[Viewer 1] B -->|rtmp://server/watch/def| D[Viewer 2] B -->|rtmp://server/watch/def| E[Viewer 3] This is achieved with the following configuration workflow restricted { rtmp_receive rtmp_app=publish stream_key=abc rtmp_watch rtmp_app=watch stream_key=def }","title":"Restricted Publish / Playback By Stream Key"},{"location":"user-guide/scenarios/key_restricted/#restricted-publish-playback-by-stream-key","text":"In this scenario, we want to only allow one specific stream key to receive video. We want viewers to be able to connect and watch the stream, however we do not want viewers to know the stream key used to publish video (so others can't publish video as the publisher). Thus the publishing stream key is different than the watching stream key. graph TD A[Video Publisher] -->|rtmp://server/publish/abc| B[mmids] B -->|rtmp://server/watch/def| C[Viewer 1] B -->|rtmp://server/watch/def| D[Viewer 2] B -->|rtmp://server/watch/def| E[Viewer 3] This is achieved with the following configuration workflow restricted { rtmp_receive rtmp_app=publish stream_key=abc rtmp_watch rtmp_app=watch stream_key=def }","title":"Restricted Publish / Playback By Stream Key"},{"location":"user-guide/scenarios/network_restricted/","text":"Network Restricted Publishing In this scenario, we only want to allow publishers from within the local intranet, and only on RTMPS. RTMP viewers are unrestricted. graph TD A -->|rtmps://10.0.0.27/live/stream| B B -->|rtmp://server/live/stream| C B -->|rtmp://server/live/stream| D B -->|rtmp://server/live/stream| E subgraph 10.0.0.15 A[Video Publisher] end subgraph 10.0.0.27 B[mmids] end subgraph Public Internet C[Viewer 1] D[Viewer 2] E[Viewer 3] end This can be done with the following configuration: workflow network_publish { rtmp_receive rtmp_app=live stream_key=* rtmps allow_ips=10.0.0.0/24 rtmp_watch rtmp_app=live stream_key=* }","title":"Network Restricted Publish"},{"location":"user-guide/scenarios/network_restricted/#network-restricted-publishing","text":"In this scenario, we only want to allow publishers from within the local intranet, and only on RTMPS. RTMP viewers are unrestricted. graph TD A -->|rtmps://10.0.0.27/live/stream| B B -->|rtmp://server/live/stream| C B -->|rtmp://server/live/stream| D B -->|rtmp://server/live/stream| E subgraph 10.0.0.15 A[Video Publisher] end subgraph 10.0.0.27 B[mmids] end subgraph Public Internet C[Viewer 1] D[Viewer 2] E[Viewer 3] end This can be done with the following configuration: workflow network_publish { rtmp_receive rtmp_app=live stream_key=* rtmps allow_ips=10.0.0.0/24 rtmp_watch rtmp_app=live stream_key=* }","title":"Network Restricted Publishing"},{"location":"user-guide/scenarios/per_publisher/","text":"Per Publisher Workflows In this scenario, each publisher might require a different workflows. Some might require transcoding, some might require being pushed into specific RMTP endpoints, some should not be allowed to push in video at all. graph TD A[Video Publisher] B[mmids] C[Reactor] A -->|rtmp://server/live/abc| B B -->|lookup 'abc'| C C -->|respond with workflows| B B --> D[Workflow Execution] The configuration for this is: reactor lookup executor=simple_http update_interval=60 { url https://lookup-server/query } workflow receiver { rtmp_receive rtmp_app=live stream_key=* reactor=lookup workflow_forwarder reactor=lookup } Note This assumes a web server is setup as a reactor at https://lookup-server that can respond to /query requests. When a publisher requests to publish to the server on a specific stream key, that stream key will be passed in a PUT request to https://lookup-server/query , wiht the stream key as the requests body. If the stream key is not allowed the lookup server will respond with a 404 , causing the rtmp_receive step to disconnect the publisher. If the stream key is allowed, mmids will create the workflows returned by the lookup server, and the workflow_forwarder step will forward the publisher's media stream to those workflows.","title":"Per Publisher Workflows"},{"location":"user-guide/scenarios/per_publisher/#per-publisher-workflows","text":"In this scenario, each publisher might require a different workflows. Some might require transcoding, some might require being pushed into specific RMTP endpoints, some should not be allowed to push in video at all. graph TD A[Video Publisher] B[mmids] C[Reactor] A -->|rtmp://server/live/abc| B B -->|lookup 'abc'| C C -->|respond with workflows| B B --> D[Workflow Execution] The configuration for this is: reactor lookup executor=simple_http update_interval=60 { url https://lookup-server/query } workflow receiver { rtmp_receive rtmp_app=live stream_key=* reactor=lookup workflow_forwarder reactor=lookup } Note This assumes a web server is setup as a reactor at https://lookup-server that can respond to /query requests. When a publisher requests to publish to the server on a specific stream key, that stream key will be passed in a PUT request to https://lookup-server/query , wiht the stream key as the requests body. If the stream key is not allowed the lookup server will respond with a 404 , causing the rtmp_receive step to disconnect the publisher. If the stream key is allowed, mmids will create the workflows returned by the lookup server, and the workflow_forwarder step will forward the publisher's media stream to those workflows.","title":"Per Publisher Workflows"},{"location":"user-guide/scenarios/simple/","text":"Simple Publish / Playback In this scenario, it is desired to have RTMP publishers send in video on any stream key, and any RTMP clients are allowed to watch those stream. No transformation of video is desired. graph TD A[Video Publisher] -->|rtmp://server/live/abc| B[mmids] B -->|rtmp://server/live/abc| C[Viewer 1] B -->|rtmp://server/live/abc| D[Viewer 2] B -->|rtmp://server/live/abc| E[Viewer 3] This is achieved with the following configuration workflow simple { rtmp_receive rtmp_app=live stream_key=* rtmp_watch rtmp_app=live stream_key=* }","title":"Simple Publish / Playback"},{"location":"user-guide/scenarios/simple/#simple-publish-playback","text":"In this scenario, it is desired to have RTMP publishers send in video on any stream key, and any RTMP clients are allowed to watch those stream. No transformation of video is desired. graph TD A[Video Publisher] -->|rtmp://server/live/abc| B[mmids] B -->|rtmp://server/live/abc| C[Viewer 1] B -->|rtmp://server/live/abc| D[Viewer 2] B -->|rtmp://server/live/abc| E[Viewer 3] This is achieved with the following configuration workflow simple { rtmp_receive rtmp_app=live stream_key=* rtmp_watch rtmp_app=live stream_key=* }","title":"Simple Publish / Playback"},{"location":"user-guide/scenarios/simple_rtmp_load_balancing/","text":"Simple RTMP Load Balancing In this scenario we want to serve many RMTP clients for a single stream, but don't want to overload the ingestion server. We want to be able to add new playback nodes on-demand. graph TD A[Video Publisher] B[Ingestion mmids] C1[Playback mmids 1] C2[Playback mmids 2] C3[Playback mmids 3] LB[Load Balancer] V1[Viewer] V2[Viewer] V3[Viewer] V4[Viewer] V5[Viewer] A -->|rtmp://ingest-server/live/stream| B B --> C1 B -->|pull rtmp://ingest-server/live/stream| C2 B --> C3 C1 --> LB C2 --> LB C3 --> LB LB --> V1 LB --> V2 LB -->|rtmp://load-balancer/live/watch| V3 LB --> V4 LB --> V5 The configuration for the ingestion mmids instance would be: workflow ingestion { rtmp_receive rtmp_app=live stream_key=stream rtmp_watch rtmp_app=live stream_key=stream } The configuration for each playback mmids instance would be: workflow playback { ffmpeg_pull location=rtmp://ingest-server/live/stream stream_name=watch rtmp_watch rtmp_app=live stream_key=watch }","title":"Simple RTMP Load Balancing"},{"location":"user-guide/scenarios/simple_rtmp_load_balancing/#simple-rtmp-load-balancing","text":"In this scenario we want to serve many RMTP clients for a single stream, but don't want to overload the ingestion server. We want to be able to add new playback nodes on-demand. graph TD A[Video Publisher] B[Ingestion mmids] C1[Playback mmids 1] C2[Playback mmids 2] C3[Playback mmids 3] LB[Load Balancer] V1[Viewer] V2[Viewer] V3[Viewer] V4[Viewer] V5[Viewer] A -->|rtmp://ingest-server/live/stream| B B --> C1 B -->|pull rtmp://ingest-server/live/stream| C2 B --> C3 C1 --> LB C2 --> LB C3 --> LB LB --> V1 LB --> V2 LB -->|rtmp://load-balancer/live/watch| V3 LB --> V4 LB --> V5 The configuration for the ingestion mmids instance would be: workflow ingestion { rtmp_receive rtmp_app=live stream_key=stream rtmp_watch rtmp_app=live stream_key=stream } The configuration for each playback mmids instance would be: workflow playback { ffmpeg_pull location=rtmp://ingest-server/live/stream stream_name=watch rtmp_watch rtmp_app=live stream_key=watch }","title":"Simple RTMP Load Balancing"},{"location":"user-guide/scenarios/smart_rtmp_load_balancing/","text":"Dynamic RTMP Load Balancing In this scenario we have a set of ingestion mmids instances and a set of playback mmids instances, with each set being able to independently scale. Video publishers should be able to publish to any ingestion instance with any stream key, and any playback client should be able to any playback instance. A playback instance should only pull an video stream if there is at least one playback connected on that instance, and it should pull from the correct ingestion instance. graph TD Pub1[Video Publisher 1] Pub2[Video Publisher 2] I1[Ingestion mmids 1] I2[Ingestion mmids 2] P1[Playback mmids 1] P2[Playback mmids 2] P3[Playback mmids 3] LBI[Ingestion Load Balancer] LBV[Viewer Load Balancer] V1[Viewer 1] V2[Viewer 2] V3[Viewer 3] Pub1 --> LBI Pub2 --> LBI LBI --> I1 LBI --> I2 I1 --> P1 I1 --> P2 I1 --> P3 I2 --> P1 I2 --> P2 I2 --> P3 P1 --> LBV P2 --> LBV P3 --> LBV LBV --> V1 LBV --> V2 LBV --> V3 To achieve this, the ingestion instances need to be configured with: reactor publishers executor=simple_http update_interval=60 { url: https://web-server/publisher_lookup } workflow ingestion { rtmp_receive rtmp_app=ingest stream_key=* reactor=publishers rtmp_watch rtmp_app=pull_source stream_key=* } The playback instances need to be configured with: reactor watchers executor=simple_http update_interval=60 { url: https://web-server/watcher_lookup } workflow pending_watchers { rtmp_watch rtmp_app=live stream_key=* } The web server handling reactor requests need to be custom coded with logic to handle both /publisher_lookup and /watcher_lookup calls. When /publisher_lookup is called, write down in a cache the ip address the HTTP request came in from. This lets us know which ip address the feed is coming in from. Once that's done we should respond with a 200 but no workflow definitions (since no additional functionality is needed from the ingestion instances). When /watcher_lookup is called, look up the stream name from the cache. If it's not in the cache (e.g. /publisher_lookup wasn't called for this stream) then return a 404 . If the stream name is in the cache, return a 200 with a workflow defined as: workflow ingest_<stream_name> { ffmpeg_pull location=rtmp://<ip_address>/pull_source/<stream_name> stream_name=<stream_name> } where <stream_name> is replaced with the name of the stream in the HTTP request, and <ip_address> was the cached IP address for the ingestion node that is being published to.","title":"Dynamic RTMP Load Balancing"},{"location":"user-guide/scenarios/smart_rtmp_load_balancing/#dynamic-rtmp-load-balancing","text":"In this scenario we have a set of ingestion mmids instances and a set of playback mmids instances, with each set being able to independently scale. Video publishers should be able to publish to any ingestion instance with any stream key, and any playback client should be able to any playback instance. A playback instance should only pull an video stream if there is at least one playback connected on that instance, and it should pull from the correct ingestion instance. graph TD Pub1[Video Publisher 1] Pub2[Video Publisher 2] I1[Ingestion mmids 1] I2[Ingestion mmids 2] P1[Playback mmids 1] P2[Playback mmids 2] P3[Playback mmids 3] LBI[Ingestion Load Balancer] LBV[Viewer Load Balancer] V1[Viewer 1] V2[Viewer 2] V3[Viewer 3] Pub1 --> LBI Pub2 --> LBI LBI --> I1 LBI --> I2 I1 --> P1 I1 --> P2 I1 --> P3 I2 --> P1 I2 --> P2 I2 --> P3 P1 --> LBV P2 --> LBV P3 --> LBV LBV --> V1 LBV --> V2 LBV --> V3 To achieve this, the ingestion instances need to be configured with: reactor publishers executor=simple_http update_interval=60 { url: https://web-server/publisher_lookup } workflow ingestion { rtmp_receive rtmp_app=ingest stream_key=* reactor=publishers rtmp_watch rtmp_app=pull_source stream_key=* } The playback instances need to be configured with: reactor watchers executor=simple_http update_interval=60 { url: https://web-server/watcher_lookup } workflow pending_watchers { rtmp_watch rtmp_app=live stream_key=* } The web server handling reactor requests need to be custom coded with logic to handle both /publisher_lookup and /watcher_lookup calls. When /publisher_lookup is called, write down in a cache the ip address the HTTP request came in from. This lets us know which ip address the feed is coming in from. Once that's done we should respond with a 200 but no workflow definitions (since no additional functionality is needed from the ingestion instances). When /watcher_lookup is called, look up the stream name from the cache. If it's not in the cache (e.g. /publisher_lookup wasn't called for this stream) then return a 404 . If the stream name is in the cache, return a 200 with a workflow defined as: workflow ingest_<stream_name> { ffmpeg_pull location=rtmp://<ip_address>/pull_source/<stream_name> stream_name=<stream_name> } where <stream_name> is replaced with the name of the stream in the HTTP request, and <ip_address> was the cached IP address for the ingestion node that is being published to.","title":"Dynamic RTMP Load Balancing"},{"location":"user-guide/steps/ffmpeg_hls/","text":"ffmpeg HLS The ffmpeg HLS step passes all media streams it receives to ffmpeg to generate an HLS playlist for each. Each media stream an HLS playlist is generated for will have a file name based on the stream name. So for example, if the video comes in via a stream key of abcd , then the resulting HLS playlist will have the filename of abcd.m3u8 . Warning ffmpeg will overwrite the HLS playlist if one already exists with the same name. Configuration The ffmpeg HLS step is utilized with the step type name of ffmpeg_hls . It supports the following arguments: path=<directory> This is a required argument that tells ffmpeg what directory to place the HLS playlist in. duration=<number> Specifies how many seconds each HLS segment should be. count=<number> Specifies the maximum number of HLS segments that should be in the HLS playlist. If the number 0 is specified, then the HLS playlist will retain all segments","title":"ffmpeg HLS"},{"location":"user-guide/steps/ffmpeg_hls/#ffmpeg-hls","text":"The ffmpeg HLS step passes all media streams it receives to ffmpeg to generate an HLS playlist for each. Each media stream an HLS playlist is generated for will have a file name based on the stream name. So for example, if the video comes in via a stream key of abcd , then the resulting HLS playlist will have the filename of abcd.m3u8 . Warning ffmpeg will overwrite the HLS playlist if one already exists with the same name.","title":"ffmpeg HLS"},{"location":"user-guide/steps/ffmpeg_hls/#configuration","text":"The ffmpeg HLS step is utilized with the step type name of ffmpeg_hls . It supports the following arguments: path=<directory> This is a required argument that tells ffmpeg what directory to place the HLS playlist in. duration=<number> Specifies how many seconds each HLS segment should be. count=<number> Specifies the maximum number of HLS segments that should be in the HLS playlist. If the number 0 is specified, then the HLS playlist will retain all segments","title":"Configuration"},{"location":"user-guide/steps/ffmpeg_pull/","text":"ffmpeg Pull The ffmpeg Pull step creates an ffmpeg process to ingest the specified media file or url into the workflow. Configuration The ffmpeg Pull step is utilized with the step type name ffmpeg_pull . It supports the following arguments: location=<path> Specifies the file path or url of the media to ingest stream_name=<name> Specifies the name the ingested media stream have internally.","title":"ffmpeg Pull"},{"location":"user-guide/steps/ffmpeg_pull/#ffmpeg-pull","text":"The ffmpeg Pull step creates an ffmpeg process to ingest the specified media file or url into the workflow.","title":"ffmpeg Pull"},{"location":"user-guide/steps/ffmpeg_pull/#configuration","text":"The ffmpeg Pull step is utilized with the step type name ffmpeg_pull . It supports the following arguments: location=<path> Specifies the file path or url of the media to ingest stream_name=<name> Specifies the name the ingested media stream have internally.","title":"Configuration"},{"location":"user-guide/steps/ffmpeg_push/","text":"ffmpeg Push The ffmpeg Push step utilizes ffmpeg to push a media stream to a target location. Warning The ffmpeg Push step does not support dynamic push targetting. If multiple media streams come into the step then they will all be sent to the target. This step is meant to be used in workflows that have a single media stream. Configuration The ffmpeg Push step can be utilized with the step type name ffmpeg_push . The supported arguments are: target=<url> The url to send the media stream to","title":"ffmpeg Push"},{"location":"user-guide/steps/ffmpeg_push/#ffmpeg-push","text":"The ffmpeg Push step utilizes ffmpeg to push a media stream to a target location. Warning The ffmpeg Push step does not support dynamic push targetting. If multiple media streams come into the step then they will all be sent to the target. This step is meant to be used in workflows that have a single media stream.","title":"ffmpeg Push"},{"location":"user-guide/steps/ffmpeg_push/#configuration","text":"The ffmpeg Push step can be utilized with the step type name ffmpeg_push . The supported arguments are: target=<url> The url to send the media stream to","title":"Configuration"},{"location":"user-guide/steps/ffmpeg_transcode/","text":"ffmpeg Transcode The ffmpeg transcode step takes all media streams that are passed into it, passes it to ffmpeg with specific transcode operations, then the transcoded resuling media streams are passed into the next steps. Configuration The ffmpeg transcode step is utliized by using the step type name ffmpeg_transcode . It supports the following arguments: vcodec=<codec> This parameter is required The video codec to transcode the video stream with. Supports: copy to keep the current media stream's video properties h264 to encode the video as h264 h264_preset=<preset> When the h264 vcodec is specified, this argument determines which video preset to use. Supported values are: ultrafast , superfast , veryfast , faster , fast , medium , slow , slower , and veryslow When the h264 codec is specified, this parameter is required . size=<width>x<height> When the h264 vcodec is specified, this argument specifies the width and height of the resulting video. If not specified then the video will retain its original size. kbps=<kbps> When the h264 vcodec is specified, this argument will attempt to constrain the bitrate of the video to the bitrate specified The value provided will be used for the min and max bitrate parameters","title":"ffmpeg Transcode"},{"location":"user-guide/steps/ffmpeg_transcode/#ffmpeg-transcode","text":"The ffmpeg transcode step takes all media streams that are passed into it, passes it to ffmpeg with specific transcode operations, then the transcoded resuling media streams are passed into the next steps.","title":"ffmpeg Transcode"},{"location":"user-guide/steps/ffmpeg_transcode/#configuration","text":"The ffmpeg transcode step is utliized by using the step type name ffmpeg_transcode . It supports the following arguments: vcodec=<codec> This parameter is required The video codec to transcode the video stream with. Supports: copy to keep the current media stream's video properties h264 to encode the video as h264 h264_preset=<preset> When the h264 vcodec is specified, this argument determines which video preset to use. Supported values are: ultrafast , superfast , veryfast , faster , fast , medium , slow , slower , and veryslow When the h264 codec is specified, this parameter is required . size=<width>x<height> When the h264 vcodec is specified, this argument specifies the width and height of the resulting video. If not specified then the video will retain its original size. kbps=<kbps> When the h264 vcodec is specified, this argument will attempt to constrain the bitrate of the video to the bitrate specified The value provided will be used for the min and max bitrate parameters","title":"Configuration"},{"location":"user-guide/steps/rtmp_receive/","text":"Rtmp Receive The RTMP receive workflow step allows RTMP clients to connect to mmids as a publisher and send video into a workflow. All media streams received by this step will have a stream name the same as the stream key the publisher sent video on. The media streams received are then passed on to subsequent steps. The step will register with the internal RTMP subsystem based on the arguments given. If the RTMP subsystem rejects the registration attempt, then the step will be in an errored state. The RTMP subsystem will usually only reject a registration if another workflow step is already registered for publishers to the port/application/stream key combination, or if registering for RTMPS connections on a port already used for RTMP (or vice versa). Configuration The RTMP Receive step is configured with the step type name of rtmp_receive . It supports the following arguments: Required Arguments rtmp_app=<name> Specifies the name of the rtmp application the step expects publishers to connect to stream_key=<key> What stream key this step should accept RTMP publishers on (relative to the specified RTMP application. The value can be given as * to accept any stream key on that RTMP application. Optional Arguments port=<number> The port number to accept RTMP connections on. If not specified port 1935 is used, unless rtmps flag is used in which case port 443 is the port used. rtmps Specifies that it will only accept connections with RTMPS. allow_ips=<ip_list> Contains one or more IP addresses or subnet masks that are allowed to publish. Multiple entries should be separated with a comma Only IPv4 addresses are supported E.g. allow_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 deny_ips=<ip_lists> Contains one or more IP addresses or subnet masks that are not allowed to publish. Multiple entries should be separated with a comma Only IPv4 addresses are supported Not allowed to be used at the same time as allow_ips . E.g. deny_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 reactor=<name> Specifies the reactor that stream keys should be validated with. When a new RTMP publisher connects, the Rtmp receive step will pass the stream key to the reactor. If the reactor returns a result specifying the stream name is not valid then the publisher will be disconnected. Error Conditions The RTMP receive step can go into an error state if the attempt to register with the RTMP subsystem is rejected. This usually happens when: The port cannot be opened due to it being in use for other (non-RTMP) purposes The port is used by the RTMP subsystem but used for RTMPS when requested to be non-RTMPS (or vice versa) The port, rtmp application, and stream key combination are already registered for publishers This includes if one workflow step registers for a wildcard but another workflow step registers for an exact stream key.","title":"Rtmp Receive"},{"location":"user-guide/steps/rtmp_receive/#rtmp-receive","text":"The RTMP receive workflow step allows RTMP clients to connect to mmids as a publisher and send video into a workflow. All media streams received by this step will have a stream name the same as the stream key the publisher sent video on. The media streams received are then passed on to subsequent steps. The step will register with the internal RTMP subsystem based on the arguments given. If the RTMP subsystem rejects the registration attempt, then the step will be in an errored state. The RTMP subsystem will usually only reject a registration if another workflow step is already registered for publishers to the port/application/stream key combination, or if registering for RTMPS connections on a port already used for RTMP (or vice versa).","title":"Rtmp Receive"},{"location":"user-guide/steps/rtmp_receive/#configuration","text":"The RTMP Receive step is configured with the step type name of rtmp_receive . It supports the following arguments: Required Arguments rtmp_app=<name> Specifies the name of the rtmp application the step expects publishers to connect to stream_key=<key> What stream key this step should accept RTMP publishers on (relative to the specified RTMP application. The value can be given as * to accept any stream key on that RTMP application. Optional Arguments port=<number> The port number to accept RTMP connections on. If not specified port 1935 is used, unless rtmps flag is used in which case port 443 is the port used. rtmps Specifies that it will only accept connections with RTMPS. allow_ips=<ip_list> Contains one or more IP addresses or subnet masks that are allowed to publish. Multiple entries should be separated with a comma Only IPv4 addresses are supported E.g. allow_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 deny_ips=<ip_lists> Contains one or more IP addresses or subnet masks that are not allowed to publish. Multiple entries should be separated with a comma Only IPv4 addresses are supported Not allowed to be used at the same time as allow_ips . E.g. deny_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 reactor=<name> Specifies the reactor that stream keys should be validated with. When a new RTMP publisher connects, the Rtmp receive step will pass the stream key to the reactor. If the reactor returns a result specifying the stream name is not valid then the publisher will be disconnected.","title":"Configuration"},{"location":"user-guide/steps/rtmp_receive/#error-conditions","text":"The RTMP receive step can go into an error state if the attempt to register with the RTMP subsystem is rejected. This usually happens when: The port cannot be opened due to it being in use for other (non-RTMP) purposes The port is used by the RTMP subsystem but used for RTMPS when requested to be non-RTMPS (or vice versa) The port, rtmp application, and stream key combination are already registered for publishers This includes if one workflow step registers for a wildcard but another workflow step registers for an exact stream key.","title":"Error Conditions"},{"location":"user-guide/steps/rtmp_watch/","text":"Rtmp Watch The RTMP watch workflow step allows RTMP clients to connect to mmmids as a playback client, and watch an existing stream. The step will register with the internal RTMP subsystem based on the arguments given. If the RTMP subsystem rejects the registration attempt, then the step will be in an errored state. The RTMP subsystem will usually only reject a registration if another workflow step is already registered for playback clients for the port/application/stream key combination, or if registering for RTMPS connections on a port already used for RTMP (or vice versa). Playback clients will not be disconnected if they initiate playback on a stream that is not active yet. The client will be held and served video when the stream becomes active. Configuration The RTMP Watch step is configured with the step type name of rtmp_watch . It supports the following arguments: Required Arguments rtmp_app=<name> Specifies the name of the rtmp application the step expects playback clients to connect to stream_key=<key> The stream key that RTMP clients can use to watch media that is actively flowing through this step. When a stream key of * is given, all media streams will be playable by RTMP clients when they connect with the same stream key as the stream name of the media stream What stream key this step should accept RTMP playback clients on (relative to the specified RTMP application. The value can be given as * to accept any stream key on that RTMP application. I Optional Arguments port=<number> The port number to accept RTMP connections on. If not specified port 1935 is used, unless rtmps flag is used in which case port 443 is the port used. rtmps Specifies that it will only accept connections with RTMPS. allow_ips=<ip_list> Contains one or more IP addresses or subnet masks that are allowed to watch. Multiple entries should be separated with a comma Only IPv4 addresses are supported E.g. allow_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 deny_ips=<ip_lists> Contains one or more IP addresses or subnet masks that are not allowed to watch. Multiple entries should be separated with a comma Only IPv4 addresses are supported Not allowed to be used at the same time as allow_ips . E.g. deny_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 reactor=<name> Specifies the reactor that stream keys should be validated with. When a new RTMP playback client connects, the Rtmp receive step will pass the stream key to the reactor. If the reactor returns a result specifying the stream name is not valid then the playback client will be disconnected. Error Conditions The RTMP receive step can go into an error state if the attempt to register with the RTMP subsystem is rejected. This usually happens when: The port cannot be opened due to it being in use for other (non-RTMP) purposes The port is used by the RTMP subsystem but used for RTMPS when requested to be non-RTMPS (or vice versa) The port, rtmp application, and stream key combination are already registered for playbakc clients This includes if one workflow step registers for a wildcard but another workflow step registers for an exact stream key.","title":"Rtmp Watch"},{"location":"user-guide/steps/rtmp_watch/#rtmp-watch","text":"The RTMP watch workflow step allows RTMP clients to connect to mmmids as a playback client, and watch an existing stream. The step will register with the internal RTMP subsystem based on the arguments given. If the RTMP subsystem rejects the registration attempt, then the step will be in an errored state. The RTMP subsystem will usually only reject a registration if another workflow step is already registered for playback clients for the port/application/stream key combination, or if registering for RTMPS connections on a port already used for RTMP (or vice versa). Playback clients will not be disconnected if they initiate playback on a stream that is not active yet. The client will be held and served video when the stream becomes active.","title":"Rtmp Watch"},{"location":"user-guide/steps/rtmp_watch/#configuration","text":"The RTMP Watch step is configured with the step type name of rtmp_watch . It supports the following arguments: Required Arguments rtmp_app=<name> Specifies the name of the rtmp application the step expects playback clients to connect to stream_key=<key> The stream key that RTMP clients can use to watch media that is actively flowing through this step. When a stream key of * is given, all media streams will be playable by RTMP clients when they connect with the same stream key as the stream name of the media stream What stream key this step should accept RTMP playback clients on (relative to the specified RTMP application. The value can be given as * to accept any stream key on that RTMP application. I Optional Arguments port=<number> The port number to accept RTMP connections on. If not specified port 1935 is used, unless rtmps flag is used in which case port 443 is the port used. rtmps Specifies that it will only accept connections with RTMPS. allow_ips=<ip_list> Contains one or more IP addresses or subnet masks that are allowed to watch. Multiple entries should be separated with a comma Only IPv4 addresses are supported E.g. allow_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 deny_ips=<ip_lists> Contains one or more IP addresses or subnet masks that are not allowed to watch. Multiple entries should be separated with a comma Only IPv4 addresses are supported Not allowed to be used at the same time as allow_ips . E.g. deny_ips=192.168.0.1,10.0.0.1,127.0.0.0/24 reactor=<name> Specifies the reactor that stream keys should be validated with. When a new RTMP playback client connects, the Rtmp receive step will pass the stream key to the reactor. If the reactor returns a result specifying the stream name is not valid then the playback client will be disconnected.","title":"Configuration"},{"location":"user-guide/steps/rtmp_watch/#error-conditions","text":"The RTMP receive step can go into an error state if the attempt to register with the RTMP subsystem is rejected. This usually happens when: The port cannot be opened due to it being in use for other (non-RTMP) purposes The port is used by the RTMP subsystem but used for RTMPS when requested to be non-RTMPS (or vice versa) The port, rtmp application, and stream key combination are already registered for playbakc clients This includes if one workflow step registers for a wildcard but another workflow step registers for an exact stream key.","title":"Error Conditions"},{"location":"user-guide/steps/workflow_forwarder/","text":"Workflow Forwarder The Workflow Forwarder step allows sending media streams to other workflows. This allows for non-linear flows for a media stream. For example, you can have a media stream forwarded to two different workflows for transcoding at different bitrates/resolutions without double transcoding. Configuration The workflow forwarder step is utilized with the workflow_forwarder step type name. The supported arguments are: target_workflow=<name> Specifies the target workflow all media streams should be forwarded to reactor=<name> Specifies the name of the reactor to check where to forward any given media stream to Each media stream will be forwarded to different workflows depending on the results of the reactor. If the reactor returns no workflows then that media stream won't be routed anywhere. Note Both arguments are not supported together. Either target_workflow or reactor can be specified, but not both.","title":"Workflow Forwarder"},{"location":"user-guide/steps/workflow_forwarder/#workflow-forwarder","text":"The Workflow Forwarder step allows sending media streams to other workflows. This allows for non-linear flows for a media stream. For example, you can have a media stream forwarded to two different workflows for transcoding at different bitrates/resolutions without double transcoding.","title":"Workflow Forwarder"},{"location":"user-guide/steps/workflow_forwarder/#configuration","text":"The workflow forwarder step is utilized with the workflow_forwarder step type name. The supported arguments are: target_workflow=<name> Specifies the target workflow all media streams should be forwarded to reactor=<name> Specifies the name of the reactor to check where to forward any given media stream to Each media stream will be forwarded to different workflows depending on the results of the reactor. If the reactor returns no workflows then that media stream won't be routed anywhere. Note Both arguments are not supported together. Either target_workflow or reactor can be specified, but not both.","title":"Configuration"}]}